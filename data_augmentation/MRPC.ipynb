{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bf43ee3-3678-4178-8d56-41a799dc924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc1bcc76-8d36-46b1-ab15-9973bf6468fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08db05ac-df25-463f-a709-f9dbcc265496",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = load_dataset(\"nyu-mll/glue\", \"mrpc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b8865b7-d042-4e93-9485-e7f30b559dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split        | Positive Pairs  | Unique Positive Groups\n",
      "-------------------------------------------------------\n",
      "train        | 2474            | 2338\n",
      "validation   | 279             | 276\n",
      "test         | 1147            | 1127\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def count_positive_groups(dataset):\n",
    "    # Build the graph: only connect sentences if they are a positive pair (label 1)\n",
    "    adj = defaultdict(set)\n",
    "    all_sentences = set()\n",
    "    \n",
    "    for row in dataset:\n",
    "        if row['label'] == 1:\n",
    "            s1, s2 = row['sentence1'], row['sentence2']\n",
    "            adj[s1].add(s2)\n",
    "            adj[s2].add(s1)\n",
    "            all_sentences.add(s1)\n",
    "            all_sentences.add(s2)\n",
    "    \n",
    "    # Traverse the graph to find connected components\n",
    "    visited = set()\n",
    "    num_groups = 0\n",
    "    \n",
    "    for sentence in all_sentences:\n",
    "        if sentence not in visited:\n",
    "            # Found a new group!\n",
    "            num_groups += 1\n",
    "            # Standard BFS to mark all sentences in this group as visited\n",
    "            queue = deque([sentence])\n",
    "            visited.add(sentence)\n",
    "            while queue:\n",
    "                current = queue.popleft()\n",
    "                for neighbor in adj[current]:\n",
    "                    if neighbor not in visited:\n",
    "                        visited.add(neighbor)\n",
    "                        queue.append(neighbor)\n",
    "                        \n",
    "    return num_groups\n",
    "\n",
    "# --- Process all splits ---\n",
    "splits = ['train', 'validation', 'test']\n",
    "\n",
    "print(f\"{'Split':<12} | {'Positive Pairs':<15} | {'Unique Positive Groups'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for split_name in splits:\n",
    "    ds = load_dataset(\"nyu-mll/glue\", \"mrpc\", split=split_name)\n",
    "    \n",
    "    # Count total positive pairs for context\n",
    "    pos_pairs = sum(1 for row in ds if row['label'] == 1)\n",
    "    \n",
    "    # Calculate transitive groups\n",
    "    groups = count_positive_groups(ds)\n",
    "    \n",
    "    print(f\"{split_name:<12} | {pos_pairs:<15} | {groups}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3b48b52-753f-4fe9-b029-8838a97585a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results for TRAIN ===\n",
      "Total Missing Pairs to achieve Group Transitivity: 149\n",
      "Largest Group Gap: Size 5 has 4/10 pairs (Missing 6)\n",
      "----------------------------------------\n",
      "=== Results for VALIDATION ===\n",
      "Total Missing Pairs to achieve Group Transitivity: 3\n",
      "Largest Group Gap: Size 3 has 2/3 pairs (Missing 1)\n",
      "----------------------------------------\n",
      "=== Results for TEST ===\n",
      "Total Missing Pairs to achieve Group Transitivity: 20\n",
      "Largest Group Gap: Size 3 has 2/3 pairs (Missing 1)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def analyze_group_completeness(dataset):\n",
    "    adj = defaultdict(set)\n",
    "    existing_positives = set()\n",
    "    all_sentences = set()\n",
    "    \n",
    "    for row in dataset:\n",
    "        if row['label'] == 1:\n",
    "            s1, s2 = row['sentence1'], row['sentence2']\n",
    "            adj[s1].add(s2)\n",
    "            adj[s2].add(s1)\n",
    "            existing_positives.add(frozenset([s1, s2]))\n",
    "            all_sentences.add(s1)\n",
    "            all_sentences.add(s2)\n",
    "            \n",
    "    visited = set()\n",
    "    total_missing_pairs = 0\n",
    "    groups_data = []\n",
    "\n",
    "    for sentence in all_sentences:\n",
    "        if sentence not in visited:\n",
    "            # 1. Identify the group (Connected Component)\n",
    "            component = []\n",
    "            queue = deque([sentence])\n",
    "            visited.add(sentence)\n",
    "            while queue:\n",
    "                curr = queue.popleft()\n",
    "                component.append(curr)\n",
    "                for neighbor in adj[curr]:\n",
    "                    if neighbor not in visited:\n",
    "                        visited.add(neighbor)\n",
    "                        queue.append(neighbor)\n",
    "            \n",
    "            # 2. Calculate completeness for this group\n",
    "            n = len(component)\n",
    "            if n > 1:\n",
    "                # Total pairs needed for a full clique: nC2\n",
    "                required_pairs = (n * (n - 1)) // 2\n",
    "                \n",
    "                # Count how many pairs actually exist in this component\n",
    "                actual_pairs = 0\n",
    "                for i in range(n):\n",
    "                    for j in range(i + 1, n):\n",
    "                        if frozenset([component[i], component[j]]) in existing_positives:\n",
    "                            actual_pairs += 1\n",
    "                \n",
    "                missing = required_pairs - actual_pairs\n",
    "                total_missing_pairs += missing\n",
    "                groups_data.append({\n",
    "                    \"size\": n,\n",
    "                    \"actual\": actual_pairs,\n",
    "                    \"required\": required_pairs,\n",
    "                    \"missing\": missing\n",
    "                })\n",
    "                \n",
    "    return total_missing_pairs, groups_data\n",
    "\n",
    "# --- Execution ---\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dt = load_dataset(\"nyu-mll/glue\", \"mrpc\", split=split)\n",
    "    total_missing, details = analyze_group_completeness(dt)\n",
    "    \n",
    "    print(f\"=== Results for {split.upper()} ===\")\n",
    "    print(f\"Total Missing Pairs to achieve Group Transitivity: {total_missing}\")\n",
    "    \n",
    "    # Show the biggest/most incomplete group as an example\n",
    "    if details:\n",
    "        worst_group = max(details, key=lambda x: x['missing'])\n",
    "        print(f\"Largest Group Gap: Size {worst_group['size']} has {worst_group['actual']}/{worst_group['required']} pairs (Missing {worst_group['missing']})\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e846b5a8-bb70-4feb-98c3-9365a68ac2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 3668\n",
      "Rows containing at least one duplicate sentence: 527\n",
      "Percentage of dataset impacted: 14.37%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1. Create a pool of all sentences\n",
    "all_sentences = dt['sentence1'] + dt['sentence2']\n",
    "\n",
    "# 2. Identify sentences that appear more than once\n",
    "counts = Counter(all_sentences)\n",
    "duplicate_sentences = {s for s, count in counts.items() if count > 1}\n",
    "\n",
    "# 3. Count rows that contain at least one of these sentences\n",
    "rows_with_duplicates = 0\n",
    "for row in dt:\n",
    "    if row['sentence1'] in duplicate_sentences or row['sentence2'] in duplicate_sentences:\n",
    "        rows_with_duplicates += 1\n",
    "\n",
    "print(f\"Total rows in dataset: {len(dt)}\")\n",
    "print(f\"Rows containing at least one duplicate sentence: {rows_with_duplicates}\")\n",
    "print(f\"Percentage of dataset impacted: {(rows_with_duplicates / len(dt)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dde8944e-7c67-4ecd-9870-8bad6eaf0a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split        | Positive (1) | Negative (0) | Total    | % Positive\n",
      "----------------------------------------------------------------------\n",
      "train        | 2474         | 1194         | 3668     |     67.45%\n",
      "validation   | 279          | 129          | 408      |     68.38%\n",
      "test         | 1147         | 578          | 1725     |     66.49%\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the splits to analyze\n",
    "splits = ['train', 'validation', 'test']\n",
    "\n",
    "print(f\"{'Split':<12} | {'Positive (1)':<12} | {'Negative (0)':<12} | {'Total':<8} | {'% Positive':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for split_name in splits:\n",
    "    # Load the specific split\n",
    "    dataset = load_dataset(\"nyu-mll/glue\", \"mrpc\", split=split_name)\n",
    "    \n",
    "    # Count occurrences of each label\n",
    "    # label 1 = paraphrase (positive), label 0 = not paraphrase (negative)\n",
    "    pos_count = sum(1 for x in dataset if x['label'] == 1)\n",
    "    neg_count = sum(1 for x in dataset if x['label'] == 0)\n",
    "    total = len(dataset)\n",
    "    \n",
    "    # Calculate percentage (handling division by zero just in case)\n",
    "    labeled_total = pos_count + neg_count\n",
    "    pos_percentage = (pos_count / labeled_total * 100) if labeled_total > 0 else 0\n",
    "    \n",
    "    print(f\"{split_name:<12} | {pos_count:<12} | {neg_count:<12} | {total:<8} | {pos_percentage:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63afa4a-4f69-4201-a69a-a31c5d77807a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
